{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Inference Classification Framework\n",
    "\n",
    "This notebook will review results from the reverse inference classification framwork. First, the methods. These are detailed in my semantic comparison google doc, and will be re-iterated here for clarity.\n",
    "\n",
    "#### Reverse Inference for Image Classification\n",
    "\n",
    "We now want to calculate the `P(cognitive process | spatial pattern of activation)` for one or more query images. The high level idea is that the query images themselves will be tagged with concepts, each tag an assertion made by the experimenter that the statistical map represents / involves the cognitive process. We would want to be able to map a single query image across the entire tree, and calculate the `P(cognitive process | the query image activation)`. While this value by itself may not be useful (or maybe it is, I’m not sure), if we compare it to our prior understnding (eg, `P(mental process)`) we can derive a computational score that tells us whether the map:\n",
    "\n",
    "- adds evidence in support of the cognitive process\n",
    "- does not contribute additional meaningful information\n",
    "- adds evidence counter to the cognitive process\n",
    "\n",
    "To achieve this, we will calculate the bayes factor that compares the prior (P(mental process) and the reverse inference score for a query image:\n",
    "\n",
    "      Bayes Factor = reverse inference score for single query image\n",
    "                         _____________________________________\n",
    "                         prior probability for a cognitive process (concept node)\n",
    "\n",
    "\n",
    ">> A value >> 1 would indicate that the image adds value\n",
    "A value == 1 would indicate that the image adds no additional information to the concept node\n",
    "A value << 1 would indicate that the image serves as evidence against (?) right way of saying that (?) the cognitive process\n",
    "\n",
    "\n",
    "We will call this score a “cognitive evidence” `(CE) score`.\n",
    "\n",
    "Given a `CE score` that compares the `P(mental process)` to the updated `P(mental process | this new map)` at every single node, we can then assess the extent to which our labeling of the node (with concepts) agrees or disagrees. For example, an image tagged with “episodic memory” and “right button click” should serve to contribute evidence to those concept nodes. If we find that the image has higher `CE` for other concept nodes, this is evidence that the image is possibly missing tags. If we find that an image has a `CE score` << 1 for a node, it suggests that the image serves as evidence against the cognitive process. We can now properly define an experiment to test this:\n",
    "\n",
    "\n",
    "###### Leave one out cross validation procedure:\n",
    "- start with database of tagged NeuroVault images, this is our “concept world”\n",
    "- remove one image from the dataset (the one guy left out! He’s so lonely!)\n",
    "- With the remaining images, generate the entire tree\n",
    "    - tree structure\n",
    "    - likelihoods\n",
    "\n",
    "- For each query image:\n",
    "  - Condition A: do classification procedure with “correct/real” tags\n",
    "  - Condition B: do classification procedure with randomly selected tags (not included in correct set)\n",
    "\n",
    "For each of the above: calculate a `CE score` for nodes for which the image is tagged (either correct in condition A, or incorrect in condition B) at the node (should we do something with child nodes too?) Section XX Calculation of CE Score for Query Image Procedure)\n",
    "\n",
    "###### Calculate accuracy:\n",
    "- For Condition A: given that the tags are correct, we would want to see scores of 1 or more, meaning that the image contributes evidence for the concept.\n",
    "- For Condition B, it’s totally random (from other tags) and so the scores should indicate that the image does not contribute evidence for the concept.\n",
    "\n",
    "I’m not sure about a final “accuracy metric,” but it makes sense to plot the scores for the “correct labels” vs. “incorrect/random.” If there is any meaningful signal / value in this procedure, we would hope to see Condition A scores >> Condition B scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
